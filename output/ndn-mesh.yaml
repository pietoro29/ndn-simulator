
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ndn-setup-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ndn-setup-role
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create", "delete", "get", "list", "patch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ndn-setup-rb
subjects:
- kind: ServiceAccount
  name: ndn-setup-sa
roleRef:
  kind: Role
  name: ndn-setup-role
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: batch/v1
kind: Job
metadata:
  name: ndn-setup-job
spec:
  ttlSecondsAfterFinished: 600
  template:
    spec:
      serviceAccountName: ndn-setup-sa
      containers:
      - name: key-generator
        image: icekarinn/ndn-all:v1.0
        command: ["/bin/bash", "-c"]
        args:
        - |
          set -e
          echo "=== Starting NDN Certificate Chain Generation ==="

          #kubectlのインストール
          apt-get update && apt-get install -y curl
          curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          mv kubectl /usr/local/bin/

          mkdir -p /work/keys
          export HOME=/root

          ROOT_PREFIX="/ndn/jp/waseda"
          SITE_PREFIX="/ndn/jp/waseda/sim-site"
          OP_PREFIX="/ndn/jp/waseda/sim-site/%C1.Operator/op"

          echo "--- 1. Root CA Generation ---"
          ndnsec key-gen $ROOT_PREFIX > root.key
          ndnsec cert-dump -i $ROOT_PREFIX > /work/keys/root.cert
          ndnsec cert-install -f /work/keys/root.cert

          echo "--- 2. Site CA Generation (Root signed) ---"
          ndnsec key-gen $SITE_PREFIX > site.key
          ndnsec cert-gen -s $ROOT_PREFIX site.key > /work/keys/site.cert
          ndnsec cert-install -f /work/keys/site.cert

          echo "--- 3. Operator CA Generation (Site signed) ---"
          ndnsec key-gen $OP_PREFIX > op.key
          ndnsec cert-gen -s $SITE_PREFIX op.key > /work/keys/op.cert
          ndnsec cert-install -f /work/keys/op.cert

          kubectl delete secret ndn-trust-anchor --ignore-not-found
          kubectl create secret generic ndn-trust-anchor \
            --from-file=/work/keys/root.cert \
            --from-file=/work/keys/site.cert \
            --from-file=/work/keys/op.cert

          echo "--- 4. Router Keys Generation (Op signed) ---"
          
          NODE_NAME="node-a"
          ROUTER_PREFIX="/ndn/jp/waseda/sim-site/%C1.Router/$NODE_NAME"

          echo "Processing $NODE_NAME ..."
          ndnsec key-gen $ROUTER_PREFIX > $NODE_NAME.key
          ndnsec cert-gen -s $OP_PREFIX $NODE_NAME.key > /work/keys/$NODE_NAME.cert
          ndnsec cert-install -f /work/keys/$NODE_NAME.cert

          ndnsec export -o /work/keys/$NODE_NAME.p12 -i $ROUTER_PREFIX -P password

          SECRET_NAME="sec-$NODE_NAME"
          kubectl delete secret $SECRET_NAME --ignore-not-found
          kubectl create secret generic $SECRET_NAME \
            --from-file=identity.p12=/work/keys/$NODE_NAME.p12

          
          NODE_NAME="node-b"
          ROUTER_PREFIX="/ndn/jp/waseda/sim-site/%C1.Router/$NODE_NAME"

          echo "Processing $NODE_NAME ..."
          ndnsec key-gen $ROUTER_PREFIX > $NODE_NAME.key
          ndnsec cert-gen -s $OP_PREFIX $NODE_NAME.key > /work/keys/$NODE_NAME.cert
          ndnsec cert-install -f /work/keys/$NODE_NAME.cert

          ndnsec export -o /work/keys/$NODE_NAME.p12 -i $ROUTER_PREFIX -P password

          SECRET_NAME="sec-$NODE_NAME"
          kubectl delete secret $SECRET_NAME --ignore-not-found
          kubectl create secret generic $SECRET_NAME \
            --from-file=identity.p12=/work/keys/$NODE_NAME.p12

          
          NODE_NAME="node-c"
          ROUTER_PREFIX="/ndn/jp/waseda/sim-site/%C1.Router/$NODE_NAME"

          echo "Processing $NODE_NAME ..."
          ndnsec key-gen $ROUTER_PREFIX > $NODE_NAME.key
          ndnsec cert-gen -s $OP_PREFIX $NODE_NAME.key > /work/keys/$NODE_NAME.cert
          ndnsec cert-install -f /work/keys/$NODE_NAME.cert

          ndnsec export -o /work/keys/$NODE_NAME.p12 -i $ROUTER_PREFIX -P password

          SECRET_NAME="sec-$NODE_NAME"
          kubectl delete secret $SECRET_NAME --ignore-not-found
          kubectl create secret generic $SECRET_NAME \
            --from-file=identity.p12=/work/keys/$NODE_NAME.p12

          

          echo "=== All Done. Secrets created. ==="
      restartPolicy: Never

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ndn-scripts
data:
  nfd.conf: |
    ; The general section contains global settings for the nfd process.
    general
    {
      ; Specify a user and/or group for NFD to drop privileges to
      ; when not performing privileged tasks. NFD does not drop
      ; privileges by default.

      ; user ndn-user
      ; group ndn-user
    }

    log
    {
      ; default_level specifies the logging level for modules
      ; that are not explicitly named. All debugging levels
      ; listed above the selected value are enabled.
      ;
      ; Valid values:
      ;
      ;  NONE ; no messages
      ;  ERROR ; error messages
      ;  WARN ; warning messages
      ;  INFO ; informational messages (default)
      ;  DEBUG ; debugging messages
      ;  TRACE ; trace messages (most verbose)
      ;  ALL ; all messages

      default_level INFO

      ; You may override default_level by assigning a logging level
      ; to the desired module name. Module names can be found in two ways:
      ;
      ; Run:
      ;   nfd --modules
      ;
      ; Or look for NFD_LOG_INIT(<module name>) statements in source files.
      ; Note that the "nfd." prefix can be omitted.
      ;
      ; Example module-level settings:
      ;
      ; FibManager DEBUG
      ; Forwarder INFO
    }

    ; The forwarder section contains settings that affect the core forwarding behavior of nfd.
    forwarder
    {
      ; Specify the HopLimit that is added to incoming Interests without a HopLimit element.
      ; A value of 0 disables adding the HopLimit.
      ; Must be between 0 and 255. The default is 0.
      default_hop_limit 0
    }

    ; The tables section configures the CS, PIT, FIB, Strategy Choice, and Measurements
    tables
    {
      ; Content Store capacity limit in number of packets.
      ; The default is 65536, equivalent to about 500MB with 8KB packet size.
      cs_max_packets 0
      ; cs_max_packets 65536

      ; Content Store replacement policy.
      ; Available policies are: priority_fifo, lru
      cs_policy lru

      ; Set a policy to decide whether to cache or drop unsolicited Data.
      ; Available policies are: drop-all, admit-local, admit-network, admit-all
      cs_unsolicited_policy drop-all

      ; Set the forwarding strategy for the specified prefixes:
      ;   <prefix> <strategy>
      strategy_choice
      {
        /               /localhost/nfd/strategy/best-route
        /localhost      /localhost/nfd/strategy/multicast
        /localhost/nfd  /localhost/nfd/strategy/best-route
        /ndn/broadcast  /localhost/nfd/strategy/multicast
      }

      ; Declare network region names
      ; These are used for mobility support.  An Interest carrying a Link object is
      ; assumed to have reached the producer region if any delegation name in the
      ; Link object is a prefix of any region name.
      network_region
      {
        ; /example/region1
        ; /example/region2
      }
    }

    ; The face_system section defines what faces and channels are created.
    face_system
    {
      ; This section contains options that apply to multiple face protocols.
      general
      {
        enable_congestion_marking yes ; set to 'no' to disable congestion marking on supported faces, default 'yes'
      }

      ; The unix section contains settings for Unix stream faces and channels.
      ; A Unix channel is always listening; delete the unix section to disable
      ; Unix stream faces and channels.
      unix
      {
        ; The default transport is unix:///run/nfd.sock (on Linux) or unix:///var/run/nfd.sock (on
        ; other platforms). This should match the "transport" field in client.conf for ndn-cxx. If you
        ; wish to use TCP instead of Unix sockets with ndn-cxx, change "transport" to an appropriate
        ; TCP FaceUri.
        path /run/nfd/nfd.sock ; Unix stream listener path
      }

      ; The tcp section contains settings for TCP faces and channels.
      tcp
      {
        listen yes ; set to 'no' to disable TCP listener, default 'yes'
        port 6363 ; TCP listener port number
        enable_v4 yes ; set to 'no' to disable IPv4 channels, default 'yes'
        enable_v6 yes ; set to 'no' to disable IPv6 channels, default 'yes'

        ; A TCP face has local scope if the local and remote IP addresses match the whitelist but not the blacklist
        local
        {
          whitelist
          {
            subnet 127.0.0.0/8
            subnet ::1/128
          }
          blacklist
          {
          }
        }
      }

      ; The udp section contains settings for UDP faces and channels.
      udp
      {
        ; UDP unicast settings.
        listen yes ; set to 'no' to disable UDP listener, default 'yes'
        port 6363 ; UDP listener port number
        enable_v4 yes ; set to 'no' to disable IPv4 channels, default 'yes'
        enable_v6 yes ; set to 'no' to disable IPv6 channels, default 'yes'

        ; Time (in seconds) before closing an idle UDP unicast face.
        ; The actual timeout will occur anytime between idle_timeout and 2*idle_timeout.
        ; The default is 600 (10 minutes).
        idle_timeout 600

        ; Maximum payload size for outgoing packets on unicast faces, used in NDNLPv2 fragmentation.
        ; This must be between 64 and 8800. The default is 8800.
        ; This value excludes IPv4/IPv6/UDP headers. On an Ethernet link of MTU=1500, setting this
        ; to 1452 would leave enough room for IP+UDP headers and prevent IP fragmentation.
        ; This option is not changable during runtime configuration reload, but the MTU of an
        ; individual face can be updated via NFD Management Protocol or the 'nfdc' tool.
        unicast_mtu 8800

        ; UDP multicast settings.
        ; By default, NFD creates one UDP multicast face per NIC.
        ;
        ; In multi-homed Linux machines these settings will NOT work without
        ; root or setting the appropriate permissions:
        ;
        ;    sudo setcap cap_net_raw=eip /path/to/nfd
        ;
        mcast yes ; set to 'no' to disable UDP multicast, default 'yes'
        mcast_group 224.0.23.170 ; UDP multicast group (IPv4)
        mcast_port 56363 ; UDP multicast port number (IPv4)
        mcast_group_v6 ff02::1234 ; UDP multicast group (IPv6)
        mcast_port_v6 56363 ; UDP multicast port number (IPv6)
        mcast_ad_hoc no ; set to 'yes' to make all UDP multicast faces "ad hoc", default 'no'

        ; Whitelist and blacklist can contain, in no particular order:
        ; - interface names, including wildcard patterns (e.g., 'ifname eth0', 'ifname en*', 'ifname wlp?s0')
        ; - MAC addresses (e.g., 'ether 85:3b:4d:d3:5f:c2')
        ; - IPv4 subnets (e.g., 'subnet 192.0.2.0/24')
        ; - IPv6 subnets (e.g., 'subnet 2001:db8::/32')
        ; - a single asterisk ('*') that matches all interfaces
        ; By default, all interfaces are whitelisted.
        whitelist
        {
          *
        }
        blacklist
        {
        }
      }

      ; The ether section contains settings for Ethernet faces and channels.
      ; These settings will NOT work without root or setting the appropriate
      ; permissions:
      ;
      ;    sudo setcap cap_net_raw,cap_net_admin=eip /path/to/nfd
      ;
      ; You may need to install a package to use setcap:
      ;
      ; **Ubuntu:**
      ;
      ;    sudo apt install libcap2-bin
      ;
      ; **Mac OS X:**
      ;
      ;    curl https://bugs.wireshark.org/bugzilla/attachment.cgi?id=3373 -o ChmodBPF.tar.gz
      ;    tar zxvf ChmodBPF.tar.gz
      ;    open ChmodBPF/Install\ ChmodBPF.app
      ;
      ; or manually:
      ;
      ;    sudo chgrp admin /dev/bpf*
      ;    sudo chmod g+rw /dev/bpf*
      ;
      ether
      {
        ; Ethernet unicast settings.
        listen yes ; set to 'no' to disable Ethernet listener, default 'yes'

        ; Time (in seconds) before closing an idle Ethernet unicast face.
        ; The actual timeout will occur anytime between idle_timeout and 2*idle_timeout.
        ; The default is 600 (10 minutes).
        idle_timeout 600

        ; Ethernet multicast settings.
        ; By default, NFD creates one Ethernet multicast face per NIC.
        mcast yes ; set to 'no' to disable Ethernet multicast, default 'yes'
        mcast_group 01:00:5E:00:17:AA ; Ethernet multicast group
        mcast_ad_hoc no ; set to 'yes' to make all Ethernet multicast faces "ad hoc", default 'no'

        ; Whitelist and blacklist can contain, in no particular order:
        ; - interface names, including wildcard patterns (e.g., 'ifname eth0', 'ifname en*', 'ifname wlp?s0')
        ; - MAC addresses (e.g., 'ether 85:3b:4d:d3:5f:c2')
        ; - IPv4 subnets (e.g., 'subnet 192.0.2.0/24')
        ; - IPv6 subnets (e.g., 'subnet 2001:db8::/32')
        ; - a single asterisk ('*') that matches all interfaces
        ; By default, all interfaces are whitelisted.
        whitelist
        {
          *
        }
        blacklist
        {
        }
      }

      ; The websocket section contains settings for WebSocket faces and channels.
      websocket
      {
        listen yes ; set to 'no' to disable WebSocket listener, default 'yes'
        port 9696 ; WebSocket listener port number
        enable_v4 yes ; set to 'no' to disable listening on IPv4 socket, default 'yes'
        enable_v6 yes ; set to 'no' to disable listening on IPv6 socket, default 'yes'
      }

      ; The netdev_bound section defines faces bound to netdevices.
      netdev_bound
      {
        ; A rule consists of a whitelist, a blacklist, and a set of remote FaceUris, and will cause the
        ; creation of zero or more faces bound to netdevices. One face will be created per accepted
        ; netdev per remote. There can be any number of rules in the netdev_bound section.

        ; rule
        ; {
        ;   ; Remote FaceUri to which the netdev-bound faces will connect.
        ;   ; Rule can contain multiple remotes. One face will be created for each remote.
        ;   ; All FaceUris must be in canonical form. Currently only udp4 and udp6 are supported.
        ;   remote udp4://192.0.2.1:6363
        ;
        ;   ; Whitelist and blacklist can contain, in no particular order:
        ;   ; - interface names, including wildcard patterns (e.g., 'ifname eth0', 'ifname en*', 'ifname wlp?s0')
        ;   ; - MAC addresses (e.g., 'ether 85:3b:4d:d3:5f:c2')
        ;   ; - IPv4 subnets (e.g., 'subnet 192.0.2.0/24')
        ;   ; - IPv6 subnets (e.g., 'subnet 2001:db8::/32')
        ;   ; - a single asterisk ('*') that matches all interfaces
        ;   ; By default, all interfaces are whitelisted.
        ;   whitelist
        ;   {
        ;     *
        ;   }
        ;   blacklist
        ;   {
        ;   }
        ; }
      }
    }

    ; The authorizations section grants privileges to authorized keys.
    authorizations
    {
      ; An authorize section grants privileges to a NDN certificate.
      authorize
      {
        ; If you do not already have NDN certificate, you can generate
        ; one with the following commands.
        ;
        ; 1. Generate and install a self-signed identity certificate:
        ;
        ;      ndnsec key-gen /$(whoami) | ndnsec cert-install -
        ;
        ; Note that the argument to 'ndnsec key-gen' will be the identity name of
        ; the new key (in this case, /your-username). Identities are hierarchical
        ; NDN names and may have multiple components (e.g., /ndn/ucla/edu/alice).
        ; You may create additional keys and identities as needed.
        ;
        ; 2. Export the certificate to a file:
        ;
        ;      ndnsec cert-dump -i /$(whoami) > default.ndncert
        ;      sudo mkdir -p /usr/local/etc/ndn/keys
        ;      sudo mv default.ndncert /usr/local/etc/ndn/keys/default.ndncert
        ;
        ; The "certfile" field below specifies the default key directory for
        ; your machine. You may move your newly created key to the location it
        ; specifies or path.

        ; certfile keys/default.ndncert ; NDN identity certificate file
        certfile any ; "any" authorizes command interests signed under any certificate,
                     ; i.e., no actual validation.
        privileges ; set of privileges granted to this identity
        {
          faces
          fib
          cs
          strategy-choice
        }
      }

      ; You may have multiple authorize sections that specify additional
      ; certificates and their privileges.

      ; authorize
      ; {
      ;   certfile keys/this_cert_does_not_exist.ndncert
      ;   authorize
      ;   privileges
      ;   {
      ;     faces
      ;   }
      ; }
    }

    rib
    {
      ; The following localhost_security allows anyone to register routing entries in local RIB
      localhost_security
      {
        trust-anchor
        {
          type any
        }
      }

      ; localhop_security should be enabled when NFD runs on a hub.
      ; "/localhop/nfd/fib" command prefix will be disabled when localhop_security section is missing.
      localhop_security
      {
        ; This section defines the trust model for NFD RIB Management. It consists of rules and
        ; trust-anchors, which are briefly defined in this file.  For more information refer to
        ; validator configuration file format documentation:
        ;
        ;    https://docs.named-data.net/ndn-cxx/current/tutorials/security-validator-config.html
        ;
        ; A trust-anchor is a pre-trusted certificate.  This can be any certificate that is the
        ; root of certification chain (e.g., NDN testbed root certificate) or an existing
        ; default system certificate `default.ndncert`.
        ;
        ; A rule defines conditions a valid packet MUST have. A packet must satisfy one of the
        ; rules defined here. A rule can be broken into two parts: matching & checking. A packet
        ; will be matched against rules from the first to the last until a matched rule is
        ; encountered. The matched rule will be used to check the packet. If a packet does not
        ; match any rule, it will be treated as invalid.  The matching part of a rule consists
        ; of `for` and `filter` sections. They collectively define which packets can be checked
        ; with this rule. `for` defines packet type (data or interest) and `filter` defines
        ; conditions on other properties of a packet. Right now, you can only define conditions
        ; on packet name, and you can only specify ONLY ONE filter for packet name.  The
        ; checking part of a rule consists of `checker`, which defines the conditions that a
        ; VALID packet MUST have. See comments in checker section for more details.

        rule
        {
          id "RIB Command Interest"
          for interest
          ; match Commmand Interest name (either in v0.3 or v0.2 format)
          filter
          {
            type name
            regex ^<localhop><nfd><rib>[<register><unregister>]<>{1,3}$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            ; KeyLocator must be either a key name or a certificate name
            key-locator
            {
              type name
              regex ^<>*<KEY><>{1,3}$
            }
          }
        }
        rule
        {
          id "NDN Testbed Certificate Hierarchy"
          for data
          ; match certificate name only
          filter
          {
            type name
            regex ^<>*<KEY><>{3}$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              ; issuer subject name must be a prefix of issued certificate name
              hyper-relation
              {
                k-regex ^(<>*)<KEY><>{1,3}$
                k-expand \\1
                h-relation is-prefix-of
                p-regex ^(<>*)$
                p-expand \\1
              }
            }
          }
        }
        trust-anchor
        {
          type file
          ; certificate path, relative to this config file
          file-name keys/default.ndncert
        }
        ; trust-anchor entry may be repeated to specify multiple trust anchors
      }

      ; The following localhop_security should be enabled when NFD runs on a hub,
      ; which accepts all remote registrations and is a short-term solution.
      localhop_security
      {
        trust-anchor
        {
          type any
        }
      }

      ; The following prefix_announcement_validation accepts any prefix announcement
      prefix_announcement_validation
      {
        trust-anchor
        {
          type any
        }
      }

      ;  auto_prefix_propagate
      ;  {
      ;    cost 15 ; forwarding cost of prefix registered on remote router
      ;    timeout 10000 ; timeout (in milliseconds) of prefix registration command for propagation
      ;
      ;    refresh_interval 300 ; interval (in seconds) before refreshing the propagation
      ;    ; This setting should be less than face_system.udp.idle_time,
      ;    ; so that the face is kept alive on the remote router.
      ;
      ;    base_retry_wait 50 ; base wait time (in seconds) before retrying propagation
      ;    max_retry_wait 3600 ; maximum wait time (in seconds) before retrying propagation
      ;    ; for consequent retries, the wait time before each retry is calculated based on the back-off
      ;    ; policy. Initially, the wait time is set to base_retry_wait, then it will be doubled for every
      ;    ; retry unless beyond the max_retry_wait, in which case max_retry_wait is set as the wait time.
      ;  }

      ; If enabled, routes registered with origin=client (typically from auto_prefix_propagate)
      ; will be readvertised into local NLSR daemon.
      readvertise_nlsr no
    }

  entrypoint.sh: |
    #!/bin/bash
    set -e

    NODE_NAME=$1
    echo "=== Initializing Node: ${NODE_NAME} ==="
    mkdir -p /usr/local/etc/ndn
    cp /scripts/nfd.conf /usr/local/etc/ndn/nfd.conf
    mkdir -p /etc/nlsr
    cp /bootstrap/nlsr/nlsr.conf /etc/nlsr/nlsr.conf

    mkdir -p /run/nfd
    export HOME=/root

    if [ "$SECURITY_MODE" == "strict" ]; then
        echo "--- [Strict] Importing Identity from Secret ---"
        if [ ! -f /etc/secret/identity.p12 ]; then
            echo "Error: Identity file not found. Setup Job might be running."
            exit 1
        fi
        ndnsec import -P password /etc/secret/identity.p12

        NETWORK_PREFIX="/ndn/jp/waseda"
        SITE_NAME="sim-site"
        ROUTER_PREFIX="${NETWORK_PREFIX}/${SITE_NAME}/%C1.Router/${NODE_NAME}"

        ndnsec set-default "$ROUTER_PREFIX"
        ndnsec cert-dump -i "$ROUTER_PREFIX" > /etc/nlsr/router.cert

    else
        echo "--- [Lax] Generating Self-Signed Identity ---"
        NETWORK_PREFIX="/ndn/jp/waseda"
        SITE_NAME="sim-site"
        ROUTER_PREFIX="${NETWORK_PREFIX}/${SITE_NAME}/%C1.Router/${NODE_NAME}"

        ndnsec key-gen "$ROUTER_PREFIX" | ndnsec cert-install -
        ndnsec set-default "$ROUTER_PREFIX"
        ndnsec cert-dump -i "$ROUTER_PREFIX" > /etc/nlsr/router.cert
    fi

    echo "--- Starting NFD ---"
    nfd-start > /var/log/nfd.log 2>&1 &
    NFD_PID=$!
    sleep 5

    echo "Waiting for NFD socket..."
    for i in {1..30}; do
        if [ -S /run/nfd/nfd.sock ]; then
            echo "NFD is ready."
            break
        fi
        sleep 2
    done

    echo "--- Creating Faces ---"
    IFS=',' read -ra ADDR <<< "$NEIGHBORS"
    for neighbor_pair in "${ADDR[@]}"; do
        neighbor_host="${neighbor_pair%%:*}"
        if [ ! -z "$neighbor_host" ]; then
            echo "Attempting to connect to ${neighbor_host}..."
            for i in {1..15}; do
                if nfdc face create tcp://${neighbor_host}; then
                    echo "Success: Connected to ${neighbor_host}"
                    break
                else
                    sleep 2
                fi
            done
        fi
    done

    sleep 2

    echo "--- Starting NLSR ---"
    nlsr -f /etc/nlsr/nlsr.conf &
    NLSR_PID=$!

    tail -f /var/log/nfd.log &
    wait $NFD_PID $NLSR_PID
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-node-a
data:
  nlsr.conf: |
    general
    {
      network /ndn/jp/waseda
      site sim-site
      router /%C1.Router/node-a

      lsa-refresh-time 1800
      lsa-interest-lifetime 4
      sync-protocol psync
      sync-interest-lifetime 60000
      state-dir       /var/lib/nlsr
    }

    neighbors
    {
       hello-retries 3
       hello-timeout 1
       hello-interval  60
      adj-lsa-build-interval 10

      face-dataset-fetch-tries 3
      face-dataset-fetch-interval 3600

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-b
        face-uri "tcp://node-b"
        link-cost 10
      }

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-c
        face-uri "tcp://node-c"
        link-cost 25
      }

    }

    hyperbolic
    {
      state off
      radius   123.456
      angle    1.45,2.36
    }

    fib
    {
      max-faces-per-prefix 3
      routing-calc-interval 15
    }

    advertising
    {
      prefix /ndn/jp/waseda/sim-site/node-a
    }

    security
    {
      validator
      {
        rule
        {
          id "NLSR Hello Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><INFO>]*<nlsr><INFO><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr><INFO>]*)<nlsr><INFO><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR LSA Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><LSA>]*<nlsr><LSA>
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                ; the last four components in the prefix should be <lsaType><seqNo><version><segmentNo>
                p-regex ^<localhop>([^<nlsr><LSA>]*)<nlsr><LSA>(<>*)<><><><>$
                p-expand \\1\\2
              }
            }
          }
        }

        rule
        {
          id "NLSR datasets"
          for data
          filter
          {
            type name
            regex ^[^<nlsr>]*<nlsr>[<lsdb><routing-table>]
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY>]*)<KEY><>{1,3}$ ; router key or certificate
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr>]*)<nlsr>[<lsdb><routing-table>]
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Exception Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY><%C1.Router>]*<%C1.Router>[^<KEY><nlsr>]*<KEY><><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><%C1.Operator>]*)<%C1.Operator>[^<KEY>]*<KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchical Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/root.cert
          
        }
      }

      prefix-update-validator
      {
        rule
        {
          id "NLSR ControlCommand Rule"
          for interest
          filter
          {
            type name
            regex ^<localhost><nlsr><prefix-update>[<advertise><withdraw>]<><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><>{1,3}$
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/site.cert
          
        }
      }

      
      cert-to-publish "router.cert"
      
      cert-to-publish "/etc/trust-anchor/root.cert"
      cert-to-publish "/etc/trust-anchor/site.cert"
      cert-to-publish "/etc/trust-anchor/op.cert"
      
      
    }
---
apiVersion: v1
kind: Service
metadata:
  name: node-a
spec:
  selector:
    app: node-a
  ports:
    - protocol: TCP
      port: 6363
      name: nfd
  clusterIP: None

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-a
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-a
  template:
    metadata:
      labels:
        app: node-a
    spec:
      containers:
        - name: ndn-node
          image: icekarinn/ndn-all:v1.0
          command: ["/bin/bash", "/scripts/entrypoint.sh", "node-a"]
          env:
            - name: NODE_NAME
              value: "node-a"
            - name: NEIGHBORS
              value: "node-b:10,node-c:25"
            - name: SECURITY_MODE
              value: "strict"
          securityContext:
            capabilities:
              add: ["NET_ADMIN"]
          volumeMounts:
            - name: scripts-vol
              mountPath: /scripts
            - name: nlsr-config-vol
              mountPath: /bootstrap/nlsr
            
            - name: secret-vol
              mountPath: /etc/secret
              readOnly: true
            - name: trust-anchor-vol
              mountPath: /etc/trust-anchor
              readOnly: true
            
      volumes:
        - name: scripts-vol
          configMap:
            name: ndn-scripts
            defaultMode: 0755
        - name: nlsr-config-vol
          configMap:
            name: config-node-a
        
        - name: secret-vol
          secret:
            secretName: sec-node-a
        - name: trust-anchor-vol
          secret:
            secretName: ndn-trust-anchor
        
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-node-b
data:
  nlsr.conf: |
    general
    {
      network /ndn/jp/waseda
      site sim-site
      router /%C1.Router/node-b

      lsa-refresh-time 1800
      lsa-interest-lifetime 4
      sync-protocol psync
      sync-interest-lifetime 60000
      state-dir       /var/lib/nlsr
    }

    neighbors
    {
       hello-retries 3
       hello-timeout 1
       hello-interval  60
      adj-lsa-build-interval 10

      face-dataset-fetch-tries 3
      face-dataset-fetch-interval 3600

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-a
        face-uri "tcp://node-a"
        link-cost 10
      }

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-c
        face-uri "tcp://node-c"
        link-cost 15
      }

    }

    hyperbolic
    {
      state off
      radius   123.456
      angle    1.45,2.36
    }

    fib
    {
      max-faces-per-prefix 3
      routing-calc-interval 15
    }

    advertising
    {
      prefix /ndn/jp/waseda/sim-site/node-b
    }

    security
    {
      validator
      {
        rule
        {
          id "NLSR Hello Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><INFO>]*<nlsr><INFO><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr><INFO>]*)<nlsr><INFO><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR LSA Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><LSA>]*<nlsr><LSA>
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                ; the last four components in the prefix should be <lsaType><seqNo><version><segmentNo>
                p-regex ^<localhop>([^<nlsr><LSA>]*)<nlsr><LSA>(<>*)<><><><>$
                p-expand \\1\\2
              }
            }
          }
        }

        rule
        {
          id "NLSR datasets"
          for data
          filter
          {
            type name
            regex ^[^<nlsr>]*<nlsr>[<lsdb><routing-table>]
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY>]*)<KEY><>{1,3}$ ; router key or certificate
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr>]*)<nlsr>[<lsdb><routing-table>]
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Exception Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY><%C1.Router>]*<%C1.Router>[^<KEY><nlsr>]*<KEY><><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><%C1.Operator>]*)<%C1.Operator>[^<KEY>]*<KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchical Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/root.cert
          
        }
      }

      prefix-update-validator
      {
        rule
        {
          id "NLSR ControlCommand Rule"
          for interest
          filter
          {
            type name
            regex ^<localhost><nlsr><prefix-update>[<advertise><withdraw>]<><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><>{1,3}$
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/site.cert
          
        }
      }

      
      cert-to-publish "router.cert"
      
      
    }
---
apiVersion: v1
kind: Service
metadata:
  name: node-b
spec:
  selector:
    app: node-b
  ports:
    - protocol: TCP
      port: 6363
      name: nfd
  clusterIP: None

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-b
  template:
    metadata:
      labels:
        app: node-b
    spec:
      containers:
        - name: ndn-node
          image: icekarinn/ndn-all:v1.0
          command: ["/bin/bash", "/scripts/entrypoint.sh", "node-b"]
          env:
            - name: NODE_NAME
              value: "node-b"
            - name: NEIGHBORS
              value: "node-a:10,node-c:15"
            - name: SECURITY_MODE
              value: "strict"
          securityContext:
            capabilities:
              add: ["NET_ADMIN"]
          volumeMounts:
            - name: scripts-vol
              mountPath: /scripts
            - name: nlsr-config-vol
              mountPath: /bootstrap/nlsr
            
            - name: secret-vol
              mountPath: /etc/secret
              readOnly: true
            - name: trust-anchor-vol
              mountPath: /etc/trust-anchor
              readOnly: true
            
      volumes:
        - name: scripts-vol
          configMap:
            name: ndn-scripts
            defaultMode: 0755
        - name: nlsr-config-vol
          configMap:
            name: config-node-b
        
        - name: secret-vol
          secret:
            secretName: sec-node-b
        - name: trust-anchor-vol
          secret:
            secretName: ndn-trust-anchor
        
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-node-c
data:
  nlsr.conf: |
    general
    {
      network /ndn/jp/waseda
      site sim-site
      router /%C1.Router/node-c

      lsa-refresh-time 1800
      lsa-interest-lifetime 4
      sync-protocol psync
      sync-interest-lifetime 60000
      state-dir       /var/lib/nlsr
    }

    neighbors
    {
       hello-retries 3
       hello-timeout 1
       hello-interval  60
      adj-lsa-build-interval 10

      face-dataset-fetch-tries 3
      face-dataset-fetch-interval 3600

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-a
        face-uri "tcp://node-a"
        link-cost 25
      }

      neighbor
      {
        name /ndn/jp/waseda/sim-site/%C1.Router/node-b
        face-uri "tcp://node-b"
        link-cost 15
      }

    }

    hyperbolic
    {
      state off
      radius   123.456
      angle    1.45,2.36
    }

    fib
    {
      max-faces-per-prefix 3
      routing-calc-interval 15
    }

    advertising
    {
      prefix /ndn/jp/waseda/sim-site/node-c
    }

    security
    {
      validator
      {
        rule
        {
          id "NLSR Hello Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><INFO>]*<nlsr><INFO><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr><INFO>]*)<nlsr><INFO><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR LSA Rule"
          for data
          filter
          {
            type name
            regex ^[^<nlsr><LSA>]*<nlsr><LSA>
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><nlsr>]*)<nlsr><KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                ; the last four components in the prefix should be <lsaType><seqNo><version><segmentNo>
                p-regex ^<localhop>([^<nlsr><LSA>]*)<nlsr><LSA>(<>*)<><><><>$
                p-expand \\1\\2
              }
            }
          }
        }

        rule
        {
          id "NLSR datasets"
          for data
          filter
          {
            type name
            regex ^[^<nlsr>]*<nlsr>[<lsdb><routing-table>]
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY>]*)<KEY><>{1,3}$ ; router key or certificate
                k-expand \\1
                h-relation equal
                p-regex ^([^<nlsr>]*)<nlsr>[<lsdb><routing-table>]
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Exception Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY><%C1.Router>]*<%C1.Router>[^<KEY><nlsr>]*<KEY><><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              hyper-relation
              {
                k-regex ^([^<KEY><%C1.Operator>]*)<%C1.Operator>[^<KEY>]*<KEY><>{1,3}$
                k-expand \\1
                h-relation equal
                p-regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><><><>$
                p-expand \\1
              }
            }
          }
        }

        rule
        {
          id "NLSR Hierarchical Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/root.cert
          
        }
      }

      prefix-update-validator
      {
        rule
        {
          id "NLSR ControlCommand Rule"
          for interest
          filter
          {
            type name
            regex ^<localhost><nlsr><prefix-update>[<advertise><withdraw>]<><><>$
          }
          checker
          {
            type customized
            sig-type ecdsa-sha256
            key-locator
            {
              type name
              regex ^([^<KEY><%C1.Router>]*)<%C1.Router>[^<KEY>]*<KEY><>{1,3}$
            }
          }
        }

        rule
        {
          id "NLSR Hierarchy Rule"
          for data
          filter
          {
            type name
            regex ^[^<KEY>]*<KEY><><><>$
          }
          checker
          {
            type hierarchical
            sig-type ecdsa-sha256
          }
        }

        trust-anchor
        {
          
          type file
          file-name /etc/trust-anchor/site.cert
          
        }
      }

      
      cert-to-publish "router.cert"
      
      
    }
---
apiVersion: v1
kind: Service
metadata:
  name: node-c
spec:
  selector:
    app: node-c
  ports:
    - protocol: TCP
      port: 6363
      name: nfd
  clusterIP: None

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-c
spec:
  replicas: 1
  selector:
    matchLabels:
      app: node-c
  template:
    metadata:
      labels:
        app: node-c
    spec:
      containers:
        - name: ndn-node
          image: icekarinn/ndn-all:v1.0
          command: ["/bin/bash", "/scripts/entrypoint.sh", "node-c"]
          env:
            - name: NODE_NAME
              value: "node-c"
            - name: NEIGHBORS
              value: "node-a:25,node-b:15"
            - name: SECURITY_MODE
              value: "strict"
          securityContext:
            capabilities:
              add: ["NET_ADMIN"]
          volumeMounts:
            - name: scripts-vol
              mountPath: /scripts
            - name: nlsr-config-vol
              mountPath: /bootstrap/nlsr
            
            - name: secret-vol
              mountPath: /etc/secret
              readOnly: true
            - name: trust-anchor-vol
              mountPath: /etc/trust-anchor
              readOnly: true
            
      volumes:
        - name: scripts-vol
          configMap:
            name: ndn-scripts
            defaultMode: 0755
        - name: nlsr-config-vol
          configMap:
            name: config-node-c
        
        - name: secret-vol
          secret:
            secretName: sec-node-c
        - name: trust-anchor-vol
          secret:
            secretName: ndn-trust-anchor
        